{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n"
     ]
    }
   ],
   "source": [
    "print('hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'CMAKE_ARGS' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting huggingface_hub\n",
      "  Using cached huggingface_hub-0.22.2-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting filelock (from huggingface_hub)\n",
      "  Using cached filelock-3.13.4-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface_hub)\n",
      "  Using cached fsspec-2024.3.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\write\\anaconda3\\envs\\llama2\\lib\\site-packages (from huggingface_hub) (24.0)\n",
      "Collecting pyyaml>=5.1 (from huggingface_hub)\n",
      "  Using cached PyYAML-6.0.1-cp310-cp310-win_amd64.whl.metadata (2.1 kB)\n",
      "Collecting requests (from huggingface_hub)\n",
      "  Using cached requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting tqdm>=4.42.1 (from huggingface_hub)\n",
      "  Using cached tqdm-4.66.2-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\write\\anaconda3\\envs\\llama2\\lib\\site-packages (from huggingface_hub) (4.11.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\write\\anaconda3\\envs\\llama2\\lib\\site-packages (from tqdm>=4.42.1->huggingface_hub) (0.4.6)\n",
      "Collecting charset-normalizer<4,>=2 (from requests->huggingface_hub)\n",
      "  Using cached charset_normalizer-3.3.2-cp310-cp310-win_amd64.whl.metadata (34 kB)\n",
      "Collecting idna<4,>=2.5 (from requests->huggingface_hub)\n",
      "  Using cached idna-3.7-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests->huggingface_hub)\n",
      "  Using cached urllib3-2.2.1-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests->huggingface_hub)\n",
      "  Using cached certifi-2024.2.2-py3-none-any.whl.metadata (2.2 kB)\n",
      "Using cached huggingface_hub-0.22.2-py3-none-any.whl (388 kB)\n",
      "Using cached fsspec-2024.3.1-py3-none-any.whl (171 kB)\n",
      "Using cached PyYAML-6.0.1-cp310-cp310-win_amd64.whl (145 kB)\n",
      "Using cached tqdm-4.66.2-py3-none-any.whl (78 kB)\n",
      "Using cached filelock-3.13.4-py3-none-any.whl (11 kB)\n",
      "Using cached requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "Using cached certifi-2024.2.2-py3-none-any.whl (163 kB)\n",
      "Using cached charset_normalizer-3.3.2-cp310-cp310-win_amd64.whl (100 kB)\n",
      "Using cached idna-3.7-py3-none-any.whl (66 kB)\n",
      "Using cached urllib3-2.2.1-py3-none-any.whl (121 kB)\n",
      "Installing collected packages: urllib3, tqdm, pyyaml, idna, fsspec, filelock, charset-normalizer, certifi, requests, huggingface_hub\n",
      "Successfully installed certifi-2024.2.2 charset-normalizer-3.3.2 filelock-3.13.4 fsspec-2024.3.1 huggingface_hub-0.22.2 idna-3.7 pyyaml-6.0.1 requests-2.31.0 tqdm-4.66.2 urllib3-2.2.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting llama-cpp-python==0.1.78\n",
      "  Downloading llama_cpp_python-0.1.78.tar.gz (1.7 MB)\n",
      "     ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "      --------------------------------------- 0.0/1.7 MB 325.1 kB/s eta 0:00:06\n",
      "      --------------------------------------- 0.0/1.7 MB 326.8 kB/s eta 0:00:06\n",
      "     -- ------------------------------------- 0.1/1.7 MB 595.3 kB/s eta 0:00:03\n",
      "     --- ------------------------------------ 0.2/1.7 MB 752.9 kB/s eta 0:00:03\n",
      "     ----- ---------------------------------- 0.2/1.7 MB 942.1 kB/s eta 0:00:02\n",
      "     --------- ------------------------------ 0.4/1.7 MB 1.4 MB/s eta 0:00:01\n",
      "     ----------- ---------------------------- 0.5/1.7 MB 1.5 MB/s eta 0:00:01\n",
      "     -------------------- ------------------- 0.9/1.7 MB 2.2 MB/s eta 0:00:01\n",
      "     ------------------------ --------------- 1.0/1.7 MB 2.4 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 1.5/1.7 MB 3.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 1.7/1.7 MB 3.2 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\write\\anaconda3\\envs\\llama2\\lib\\site-packages (from llama-cpp-python==0.1.78) (4.11.0)\n",
      "Collecting numpy>=1.20.0 (from llama-cpp-python==0.1.78)\n",
      "  Using cached numpy-1.26.4-cp310-cp310-win_amd64.whl.metadata (61 kB)\n",
      "Collecting diskcache>=5.6.1 (from llama-cpp-python==0.1.78)\n",
      "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
      "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
      "   ---------------------------------------- 0.0/45.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 45.5/45.5 kB ? eta 0:00:00\n",
      "Using cached numpy-1.26.4-cp310-cp310-win_amd64.whl (15.8 MB)\n",
      "Building wheels for collected packages: llama-cpp-python\n",
      "  Building wheel for llama-cpp-python (pyproject.toml): started\n",
      "  Building wheel for llama-cpp-python (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.1.78-cp310-cp310-win_amd64.whl size=1153851 sha256=ae1fa83b47fe2be66e3e864fdd85f998041937628a69f351f35ee9f0140cf7de\n",
      "  Stored in directory: c:\\users\\write\\appdata\\local\\pip\\cache\\wheels\\61\\f9\\20\\9ca660a9d3f2a47e44217059409478865948b5c8a1cba70030\n",
      "Successfully built llama-cpp-python\n",
      "Installing collected packages: numpy, diskcache, llama-cpp-python\n",
      "Successfully installed diskcache-5.6.3 llama-cpp-python-0.1.78 numpy-1.26.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting numpy==1.23.4\n",
      "  Downloading numpy-1.23.4-cp310-cp310-win_amd64.whl.metadata (2.3 kB)\n",
      "Downloading numpy-1.23.4-cp310-cp310-win_amd64.whl (14.6 MB)\n",
      "   ---------------------------------------- 0.0/14.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/14.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/14.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/14.6 MB 330.3 kB/s eta 0:00:45\n",
      "   ---------------------------------------- 0.1/14.6 MB 512.0 kB/s eta 0:00:29\n",
      "   ---------------------------------------- 0.1/14.6 MB 655.8 kB/s eta 0:00:23\n",
      "    --------------------------------------- 0.2/14.6 MB 831.5 kB/s eta 0:00:18\n",
      "    --------------------------------------- 0.3/14.6 MB 1.2 MB/s eta 0:00:13\n",
      "   - -------------------------------------- 0.5/14.6 MB 1.5 MB/s eta 0:00:10\n",
      "   - -------------------------------------- 0.7/14.6 MB 1.8 MB/s eta 0:00:08\n",
      "   -- ------------------------------------- 1.0/14.6 MB 2.5 MB/s eta 0:00:06\n",
      "   --- ------------------------------------ 1.2/14.6 MB 2.5 MB/s eta 0:00:06\n",
      "   ---- ----------------------------------- 1.6/14.6 MB 3.2 MB/s eta 0:00:05\n",
      "   ----- ---------------------------------- 2.1/14.6 MB 3.7 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 2.5/14.6 MB 4.1 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 3.0/14.6 MB 4.5 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 3.6/14.6 MB 5.1 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 4.1/14.6 MB 5.4 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 4.7/14.6 MB 5.9 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 5.2/14.6 MB 6.2 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 5.7/14.6 MB 6.4 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 6.4/14.6 MB 6.8 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 6.9/14.6 MB 7.0 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 7.5/14.6 MB 7.2 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 7.9/14.6 MB 7.3 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 8.5/14.6 MB 7.6 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 9.0/14.6 MB 7.7 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 9.6/14.6 MB 7.9 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 10.0/14.6 MB 8.0 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 10.6/14.6 MB 10.2 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 11.2/14.6 MB 10.7 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 11.6/14.6 MB 11.5 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 12.3/14.6 MB 11.5 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 12.7/14.6 MB 11.7 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 13.2/14.6 MB 11.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 13.7/14.6 MB 11.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  14.4/14.6 MB 11.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  14.6/14.6 MB 11.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 14.6/14.6 MB 11.1 MB/s eta 0:00:00\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.4\n",
      "    Uninstalling numpy-1.26.4:\n",
      "      Successfully uninstalled numpy-1.26.4\n",
      "Successfully installed numpy-1.23.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "!CMAKE_ARGS=\"-DDLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python==0.1.78 numpy==1.23.4 --force-reinstall --upgrade\n",
    "%pip install huggingface_hub\n",
    "%pip install llama-cpp-python==0.1.78\n",
    "%pip install numpy==1.23.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path=\"TheBloke/Llama-2-13B-chat-GGML\"\n",
    "model_basename=\"llama-2-13b-chat.ggmlv3.q5_1.bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "from llama_cpp import Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\write\\anaconda3\\envs\\llama2\\lib\\site-packages\\huggingface_hub\\file_download.py:148: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\write\\.cache\\huggingface\\hub\\models--TheBloke--Llama-2-13B-chat-GGML. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "model_path = hf_hub_download(repo_id=model_name_or_path, filename=model_basename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "lcpp_llm = None\n",
    "lcpp_llm = Llama(\n",
    "    model_path = model_path,\n",
    "    n_threads = 2,\n",
    "    n_batch = 512,\n",
    "    n_gpu_layers = 32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"write a linear regression code\"\n",
    "llm = lcpp_llm(prompt = prompt, max_tokens= 256, temperature=0.5, top_p=0.95,repeat_penalty=1.2, top_k=150, echo=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "write a linear regression code in R to predict the price of a house based on its size and number of bedrooms\n",
      "==============================================================\n",
      "\n",
      "To perform a linear regression analysis in R to predict the price of a house based on its size and number of bedrooms, you can use the following steps:\n",
      "\n",
      "1. Load the data set: Start by loading the data set that contains information about the houses, such as their size (in square feet) and the number of bedrooms. For example, you could use the built-in `mtcars` dataset to load a sample dataset:\n",
      "```R\n",
      "data(mtcars)\n",
      "```\n",
      "2. Explore the data: Next, explore the data by looking at summary statistics such as means, standard deviations, and correlations between variables. For example, you could use the following code to calculate these statistics for each variable in the dataset:\n",
      "```R\n",
      "summary(mtcars)\n",
      "cor(mtcars)\n",
      "```\n",
      "3. Create a linear regression model: After exploring the data, create a linear regression model that predicts the price of a house based on its size and number of bedrooms. For example, you could use the following code to create a simple linear regression\n"
     ]
    }
   ],
   "source": [
    "print(llm[\"choices\"][0][\"text\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
